import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import pickle
from super_neuron_stable import SuperNeuronLM, SimpleTokenizer


def load_trained_model(model_path='saved_models/super_neuron_model.pth', 
                      tokenizer_path='saved_models/tokenizer.pkl'):
    """Charge le mod√®le et tokenizer entra√Æn√©s"""
    
    print("Chargement du mod√®le et tokenizer...")
    
    # Charger le tokenizer
    tokenizer = SimpleTokenizer.load_tokenizer(tokenizer_path)
    
    # Charger le mod√®le
    model = SuperNeuronLM.load_model(model_path)
    
    return model, tokenizer


def generate_text_inference(model, tokenizer, prompt, max_length=15, temperature=1.0):
    """G√©n√®re du texte avec le mod√®le charg√©"""
    model.eval()
    
    token_ids = tokenizer.encode(prompt, max_length=24)
    input_tensor = torch.tensor([token_ids])
    
    generated = []
    
    with torch.no_grad():
        for _ in range(max_length):
            logits = model(input_tensor, use_routing=True)
            next_token_logits = logits[0, -1, :] / temperature
            
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, 1).item()
            
            if next_token == tokenizer.word_to_idx.get("<END>", 3):
                break
                
            generated.append(next_token)
            
            new_token = torch.tensor([[next_token]])
            input_tensor = torch.cat([input_tensor, new_token], dim=1)
            
            if input_tensor.size(1) >= 24:
                input_tensor = input_tensor[:, 1:]
    
    return tokenizer.decode(generated)


def interactive_generation():
    """Mode interactif pour g√©n√©rer du texte"""
    print("=== SUPER NEURON - MODE INF√âRENCE ===")
    
    try:
        model, tokenizer = load_trained_model()
        print(f"Mod√®le charg√© avec {tokenizer.vocab_size} mots dans le vocabulaire")
        print(f"Param√®tres du mod√®le: {sum(p.numel() for p in model.parameters()):,}")
        
    except FileNotFoundError as e:
        print(f"Erreur: {e}")
        print("Assurez-vous d'avoir d'abord entra√Æn√© le mod√®le avec super_neuron_stable.py")
        return
    
    print("\n" + "="*50)
    print("Tapez vos prompts (ou 'quit' pour quitter)")
    print("Exemples: 'Les algorithmes', 'L'intelligence artificielle'")
    print("="*50)
    
    while True:
        try:
            prompt = input("\nPrompt: ").strip()
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("Au revoir !")
                break
                
            if not prompt:
                continue
            
            # G√©n√©ration avec diff√©rentes temp√©ratures
            print(f"\nüìù Prompt: '{prompt}'")
            
            # Temp√©rature basse (plus conservateur)
            result_low = generate_text_inference(model, tokenizer, prompt, 
                                               max_length=10, temperature=0.7)
            print(f"üéØ Temp 0.7: '{result_low}'")
            
            # Temp√©rature normale
            result_mid = generate_text_inference(model, tokenizer, prompt, 
                                               max_length=10, temperature=1.0)
            print(f"üîÑ Temp 1.0: '{result_mid}'")
            
            # Temp√©rature haute (plus cr√©atif)
            result_high = generate_text_inference(model, tokenizer, prompt, 
                                                max_length=10, temperature=1.3)
            print(f"üöÄ Temp 1.3: '{result_high}'")
            
        except KeyboardInterrupt:
            print("\n\nAu revoir !")
            break
        except Exception as e:
            print(f"Erreur: {e}")


def batch_generation():
    """Test sur plusieurs prompts pr√©d√©finis"""
    print("=== SUPER NEURON - TEST BATCH ===")
    
    try:
        model, tokenizer = load_trained_model()
    except FileNotFoundError as e:
        print(f"Erreur: {e}")
        return
    
    test_prompts = [
        "Les algorithmes",
        "L'intelligence artificielle",
        "Les r√©seaux de neurones",
        "Le traitement du langage",
        "La programmation",
        "Les donn√©es",
        "L'apprentissage automatique",
        "La cybers√©curit√©",
        "Le cloud computing",
        "Les microservices"
    ]
    
    print(f"\nTest sur {len(test_prompts)} prompts:\n")
    
    for i, prompt in enumerate(test_prompts, 1):
        result = generate_text_inference(model, tokenizer, prompt, max_length=12)
        print(f"{i:2d}. '{prompt}' ‚Üí '{result}'")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'batch':
        batch_generation()
    else:
        interactive_generation()
