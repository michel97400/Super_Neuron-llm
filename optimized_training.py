#!/usr/bin/env python3
"""
Script d'entra√Ænement optimis√© avec m√©triques, early stopping et architecture am√©lior√©e
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import json
import time
import math
from tqdm import tqdm
from collections import defaultdict
import numpy as np

# Import optionnel de matplotlib
try:
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("‚ö†Ô∏è matplotlib non disponible - graphiques d√©sactiv√©s")

# Imports locaux
from conversational_model import ConversationalLM
from optimized_architecture import OptimizedSuperNeuronConversational
from sequence_optimizer import SequenceOptimizer

class TrainingOptimizer:
    """Gestionnaire d'entra√Ænement optimis√©"""
    
    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.metrics = defaultdict(list)
        self.best_loss = float('inf')
        self.patience_counter = 0
        
    def compute_metrics(self, outputs, targets, mask=None):
        """Calcule des m√©triques d√©taill√©es"""
        with torch.no_grad():
            # Loss principale
            loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1), 
                                 reduction='none')
            
            if mask is not None:
                loss = loss * mask.view(-1)
                valid_tokens = mask.sum().item()
                loss = loss.sum() / valid_tokens
            else:
                loss = loss.mean()
            
            # Pr√©cision
            predictions = torch.argmax(outputs, dim=-1)
            correct = (predictions == targets)
            
            if mask is not None:
                correct = correct * mask
                accuracy = correct.sum().float() / mask.sum().float()
            else:
                accuracy = correct.float().mean()
            
            # Perplexit√©
            perplexity = torch.exp(loss)
            
            return {
                'loss': loss.item(),
                'accuracy': accuracy.item(),
                'perplexity': perplexity.item()
            }
    
    def train_epoch(self, dataloader, optimizer, scheduler=None):
        """Entra√Æne une √©poque avec m√©triques"""
        self.model.train()
        epoch_metrics = defaultdict(list)
        
        pbar = tqdm(dataloader, desc="Training")
        for batch_idx, batch in enumerate(pbar):
            input_ids = batch['input_ids'].to(self.device)
            targets = batch['targets'].to(self.device)
            mask = batch.get('mask', None)
            if mask is not None:
                mask = mask.to(self.device)
            
            # Forward pass
            optimizer.zero_grad()
            outputs = self.model(input_ids)
            
            # Calcul des m√©triques
            metrics = self.compute_metrics(outputs, targets, mask)
            
            # Backward pass
            loss = torch.tensor(metrics['loss'], requires_grad=True, device=self.device)
            if loss.requires_grad:
                # Recalculer la loss pour le gradient
                loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), targets.view(-1))
                if mask is not None:
                    loss = loss * mask.view(-1)
                    loss = loss.sum() / mask.sum()
                
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            if scheduler:
                scheduler.step()
            
            # Collecter m√©triques
            for key, value in metrics.items():
                epoch_metrics[key].append(value)
            
            # Mise √† jour barre de progression
            pbar.set_postfix({
                'Loss': f"{metrics['loss']:.4f}",
                'Acc': f"{metrics['accuracy']:.3f}",
                'PPL': f"{metrics['perplexity']:.2f}"
            })
        
        # Moyennes de l'√©poque
        return {key: np.mean(values) for key, values in epoch_metrics.items()}
    
    def validate(self, dataloader):
        """Validation avec m√©triques"""
        self.model.eval()
        val_metrics = defaultdict(list)
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="Validation"):
                input_ids = batch['input_ids'].to(self.device)
                targets = batch['targets'].to(self.device)
                mask = batch.get('mask', None)
                if mask is not None:
                    mask = mask.to(self.device)
                
                outputs = self.model(input_ids)
                metrics = self.compute_metrics(outputs, targets, mask)
                
                for key, value in metrics.items():
                    val_metrics[key].append(value)
        
        return {key: np.mean(values) for key, values in val_metrics.items()}
    
    def early_stopping_check(self, val_loss, patience=5):
        """V√©rification early stopping"""
        if val_loss < self.best_loss:
            self.best_loss = val_loss
            self.patience_counter = 0
            return False  # Continue training
        else:
            self.patience_counter += 1
            return self.patience_counter >= patience
    
    def save_checkpoint(self, epoch, optimizer, path="checkpoint.pth"):
        """Sauvegarde compl√®te"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_loss': self.best_loss,
            'metrics': dict(self.metrics)
        }
        torch.save(checkpoint, path)
        print(f"üíæ Checkpoint sauvegard√© : {path}")
    
    def plot_metrics(self, save_path="training_metrics.png"):
        """Graphiques des m√©triques"""
        if not HAS_MATPLOTLIB:
            print("üìä matplotlib non disponible - graphiques ignor√©s")
            return
            
        if not self.metrics:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('M√©triques d\'entra√Ænement', fontsize=16)
        
        # Loss
        axes[0,0].plot(self.metrics['train_loss'], label='Train', color='blue')
        axes[0,0].plot(self.metrics['val_loss'], label='Validation', color='red')
        axes[0,0].set_title('Loss')
        axes[0,0].set_xlabel('√âpoque')
        axes[0,0].legend()
        axes[0,0].grid(True)
        
        # Accuracy
        axes[0,1].plot(self.metrics['train_accuracy'], label='Train', color='blue')
        axes[0,1].plot(self.metrics['val_accuracy'], label='Validation', color='red')
        axes[0,1].set_title('Pr√©cision')
        axes[0,1].set_xlabel('√âpoque')
        axes[0,1].legend()
        axes[0,1].grid(True)
        
        # Perplexity
        axes[1,0].plot(self.metrics['train_perplexity'], label='Train', color='blue')
        axes[1,0].plot(self.metrics['val_perplexity'], label='Validation', color='red')
        axes[1,0].set_title('Perplexit√©')
        axes[1,0].set_xlabel('√âpoque')
        axes[1,0].legend()
        axes[1,0].grid(True)
        
        # Learning rate (si disponible)
        if 'learning_rate' in self.metrics:
            axes[1,1].plot(self.metrics['learning_rate'], color='green')
            axes[1,1].set_title('Learning Rate')
            axes[1,1].set_xlabel('√âpoque')
            axes[1,1].grid(True)
        else:
            axes[1,1].text(0.5, 0.5, 'Learning Rate\nNon disponible', 
                          ha='center', va='center', transform=axes[1,1].transAxes)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"üìä Graphiques sauvegard√©s : {save_path}")

def create_optimized_dataloader(texts, llm, sequence_optimizer, batch_size=8, max_length=100):
    """Cr√©e un dataloader optimis√©"""
    
    # Optimiser les s√©quences
    print("üîÑ Optimisation des s√©quences...")
    optimized_texts = sequence_optimizer.optimize_dataset(texts, llm)
    
    # Statistiques
    stats = sequence_optimizer.get_statistics(optimized_texts, llm)
    print(f"üìä S√©quences optimis√©es :")
    print(f"   ‚Ä¢ Nombre : {stats['count']}")
    print(f"   ‚Ä¢ Longueur moy. : {stats['avg_length']:.1f} tokens")
    print(f"   ‚Ä¢ Ratio optimal : {stats['optimal_ratio']:.1%}")
    
    # Tokenization
    sequences = []
    for text in optimized_texts:
        encoded = llm.encode(text)
        if len(encoded) >= 5:  # Minimum viable
            sequences.append(encoded)
    
    # Cr√©er batches
    batches = []
    for i in range(0, len(sequences), batch_size):
        batch_sequences = sequences[i:i+batch_size]
        
        # Padding
        max_len = min(max_length, max(len(seq) for seq in batch_sequences))
        
        batch_input_ids = []
        batch_targets = []
        batch_masks = []
        
        for seq in batch_sequences:
            # Tronquer si n√©cessaire
            if len(seq) > max_len:
                seq = seq[:max_len]
            
            # Input et target (d√©cal√© de 1)
            input_ids = seq[:-1] + [llm.vocab['<pad>']] * (max_len - len(seq))
            targets = seq[1:] + [llm.vocab['<pad>']] * (max_len - len(seq) + 1)
            mask = [1] * (len(seq) - 1) + [0] * (max_len - len(seq) + 1)
            
            batch_input_ids.append(input_ids[:max_len-1])
            batch_targets.append(targets[:max_len-1])
            batch_masks.append(mask[:max_len-1])
        
        if batch_input_ids:  # √âviter les batches vides
            batches.append({
                'input_ids': torch.tensor(batch_input_ids, dtype=torch.long),
                'targets': torch.tensor(batch_targets, dtype=torch.long),
                'mask': torch.tensor(batch_masks, dtype=torch.float)
            })
    
    return batches

def main():
    print("üöÄ === ENTRA√éNEMENT SUPER NEURON OPTIMIS√â ===")
    
    # Configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîß Device : {device}")
    
    # Charger les donn√©es
    print("\nüìö Chargement du dataset enrichi...")
    with open('conversational_dataset_enhanced.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    texts = data['conversational_texts']
    print(f"‚úÖ {len(texts)} textes charg√©s")
    
    # Pr√©parer le tokenizer
    print("\nüî§ Construction du vocabulaire...")
    llm = ConversationalLM()
    vocab_size = llm.build_vocab(texts)
    print(f"‚úÖ Vocabulaire : {vocab_size} tokens")
    
    # Optimiseur de s√©quences
    sequence_optimizer = SequenceOptimizer(
        min_length=8,
        optimal_length=60,  # Plus court pour efficacit√©
        max_length=100
    )
    
    # Pr√©parer les donn√©es
    print("\n‚öôÔ∏è Pr√©paration des donn√©es...")
    train_size = int(0.9 * len(texts))
    train_texts = texts[:train_size]
    val_texts = texts[train_size:]
    
    train_batches = create_optimized_dataloader(train_texts, llm, sequence_optimizer, batch_size=6)
    val_batches = create_optimized_dataloader(val_texts, llm, sequence_optimizer, batch_size=6)
    
    print(f"‚úÖ Train : {len(train_batches)} batches")
    print(f"‚úÖ Val : {len(val_batches)} batches")
    
    # Mod√®le optimis√©
    print("\nüß† Initialisation du mod√®le optimis√©...")
    model = OptimizedSuperNeuronConversational(
        vocab_size=vocab_size,
        embed_dim=96,  # Plus compact
        max_seq_length=100,
        num_branches=3
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"‚úÖ Param√®tres : {total_params:,}")
    
    # Optimiseur et scheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
    
    # Gestionnaire d'entra√Ænement
    trainer = TrainingOptimizer(model, device)
    
    # Entra√Ænement
    print("\nüéØ D√©but de l'entra√Ænement optimis√©...")
    epochs = 25
    
    for epoch in range(epochs):
        print(f"\nüìà √âpoque {epoch+1}/{epochs}")
        
        # Entra√Ænement
        train_metrics = trainer.train_epoch(train_batches, optimizer, scheduler)
        
        # Validation
        val_metrics = trainer.validate(val_batches)
        
        # Collecter m√©triques
        for key, value in train_metrics.items():
            trainer.metrics[f'train_{key}'].append(value)
        for key, value in val_metrics.items():
            trainer.metrics[f'val_{key}'].append(value)
        trainer.metrics['learning_rate'].append(optimizer.param_groups[0]['lr'])
        
        # Affichage
        print(f"üîµ Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.3f}, PPL: {train_metrics['perplexity']:.2f}")
        print(f"üî¥ Val   - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.3f}, PPL: {val_metrics['perplexity']:.2f}")
        
        # Early stopping
        if trainer.early_stopping_check(val_metrics['loss'], patience=8):
            print("‚èπÔ∏è Early stopping d√©clench√© !")
            break
        
        # Sauvegarde du meilleur mod√®le
        if val_metrics['loss'] == trainer.best_loss:
            trainer.save_checkpoint(epoch, optimizer, "best_optimized_model.pth")
    
    # Sauvegarde finale
    print("\nüíæ Sauvegarde finale...")
    trainer.save_checkpoint(epoch, optimizer, "final_optimized_model.pth")
    trainer.plot_metrics("optimized_training_metrics.png")
    
    # Test de g√©n√©ration
    print("\nüéØ Test de g√©n√©ration optimis√©e...")
    model.eval()
    test_prompts = ["Bonjour", "Comment allez-vous", "Qu'est-ce que"]
    
    for prompt in test_prompts:
        generated = llm.generate(prompt, max_length=15, temperature=0.8)
        print(f"'{prompt}' ‚Üí '{generated}'")
    
    print("\nüéâ Entra√Ænement optimis√© termin√© !")

if __name__ == "__main__":
    main()
